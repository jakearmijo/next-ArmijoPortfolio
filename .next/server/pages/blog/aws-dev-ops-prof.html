<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="Jake Armijo" content="Armijo - Blog"/><title>AWS DOP-C01 - Armijo - Blog</title><meta name="next-head-count" content="5"/><link rel="preload" href="/_next/static/css/304a1ce97f76cb34adde.css" as="style"/><link rel="stylesheet" href="/_next/static/css/304a1ce97f76cb34adde.css" data-n-g=""/><link rel="preload" href="/_next/static/css/212642039fea8e077473.css" as="style"/><link rel="stylesheet" href="/_next/static/css/212642039fea8e077473.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-af28476a2e7790fd48db.js" defer=""></script><script src="/_next/static/chunks/framework-2191d16384373197bc0a.js" defer=""></script><script src="/_next/static/chunks/main-1f2c591c5d3bfcfc95e6.js" defer=""></script><script src="/_next/static/chunks/pages/_app-e50ed009712380b1fb04.js" defer=""></script><script src="/_next/static/chunks/9f96d65d-7a2bc5bf0bcd1588504d.js" defer=""></script><script src="/_next/static/chunks/563-18bfd2b74e6d968f9e4b.js" defer=""></script><script src="/_next/static/chunks/528-48962624a6bcdea7064e.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bid%5D-2cf5aa60667fdadf9781.js" defer=""></script><script src="/_next/static/FxYUuXgttL70YrK1Pz9qW/_buildManifest.js" defer=""></script><script src="/_next/static/FxYUuXgttL70YrK1Pz9qW/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_container__2t4v2"><nav><a>PORTFOLIO</a><a>ABOUT</a><a>RESUME</a><a>CONTACT</a><a href="https://jakearmijo.com/blog">BLOG</a><a href="http://todaystilts.jakearmijo.com/">Today&#x27;s Tilts</a><a href="https://github.com/jakearmijo">GITHUB</a><a href="https://www.linkedin.com/in/jake-armijo/">LINKEDIN</a></nav><header class="layout_header__2rhWq"><a href="/"><div style="display:inline-block;max-width:100%;overflow:hidden;position:relative;box-sizing:border-box;margin:0"><div style="box-sizing:border-box;display:block;max-width:100%"><img style="max-width:100%;display:block;margin:0;border:none;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iODAwIiBoZWlnaHQ9IjYwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2ZXJzaW9uPSIxLjEiLz4="/></div><img alt="Jake Armijo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="layout_headerImage__2h5On utils_borderCircle__13qdJ" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Jake Armijo" srcSet="https://jakearmijo.com//_next/static/image/public/images/JAKEMIJUPDATE.26fe3eb92629b31f98ee50e930a2b37a.svg?w=828 1x, https://jakearmijo.com//_next/static/image/public/images/JAKEMIJUPDATE.26fe3eb92629b31f98ee50e930a2b37a.svg?w=1920 2x" src="https://jakearmijo.com//_next/static/image/public/images/JAKEMIJUPDATE.26fe3eb92629b31f98ee50e930a2b37a.svg?w=1920" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="layout_headerImage__2h5On utils_borderCircle__13qdJ" loading="lazy"/></noscript></div></a></header><main class="blogIntroDiv"><div class="blogIntroDiv"><h2>The Domains of Exam</h2>
<ul>
<li><strong>Domain 1: SDLC Automation (16 questions)</strong> -</li>
<li><strong>Domain 2: Configuration Management &#x26; Infrastructure as Code (16 questions)</strong> -</li>
<li><strong>Domain 3: Monitoring and Logging (11 questions)</strong> -</li>
<li><strong>Domain 4: Policies and Standards Automation (9 questions)</strong> -</li>
<li><strong>Domain 5: Incident and Event Response (13 questions)</strong> -</li>
<li><strong>Domain 6: High Availability, Fault Tolerance, &#x26; DR (10 questions)</strong> -</li>
</ul>
<h3>CD</h3>
<h4>blue/green is past but want to change to run one set of servers and to update everything at same time</h4>
<ul>
<li>Edit CD application deployment group and change the deployment type from blue/green to in-place</li>
<li>you are able to edit the application deployment group in CodeDeploy, you do not have to create a new one.</li>
</ul>
<h4>new CC repo all but 1 can connect via SSh</h4>
<ul>
<li>CC requires KMS. a policy might be attached to the already existing IAM user that DENIES KMS actions required</li>
<li>For a new IAM user attach AWSCodeCommitFullAccess or another managed policy for CC access</li>
<li>user might have prev configured his local computer to use credential helper for CC. if so EDIT the .GITCONFIG file to remove the credential helper info from the file before using git creds</li>
</ul>
<h4>RDS test DB is isolated on private subnet. Want to run at night</h4>
<ul>
<li>Amazon VPC access in your CB project needs to be enabled</li>
<li>Specify your VPC ID, subnets, and securty groups in your build project</li>
<li>set up CW Events to start pipeline on schedule</li>
<li><code>aws events put-rule --schedule-expression 'cron(10 3 ? * MON-FRI *)' --name IntegrationTests</code></li>
<li>CB DOES NOT SUPPORT assigning elastic IP addresses to the network interfaces that it creates, and auto-assigning a public IP address is not supported by EC2 for any network interfaces created outside EC2 instance launches</li>
</ul>
<h4>AWS_CODEBUILD_MAX_MEM_ALLOC defined in multiple parts</h4>
<ul>
<li>value in <strong>start build operation</strong> call takes HIGHEST precedence</li>
<li>value in the <strong>build project definition</strong></li>
<li>value in the <strong>build spec declaration</strong> takes lowest precedence</li>
<li>DO NOT SET ENV VAR with 'CODEBUILD_'</li>
</ul>
<h3>CW</h3>
<ul>
<li>With CloudTrail enabled create CW Event rule to track AWS API call via CloudTrail</li>
<li>use SNS as a target for notification</li>
<li>create a rule that triggers on an action by an AWS service that doesn not emit events, you can base the rule on API calls</li>
<li>Dynamo Streams captures information but do not capture DeleteTable API calls, they only capture item level events</li>
</ul>
<h4>Resolving the audit deficiency requires the creation of a centralized log monitoring capability for the AWS-based apps and infrastructure, and for certain on-premises systems that they interface with</h4>
<ul>
<li>Elasticsearch is a fully managed service that lets you collect and analyze logs metrics, giving you a comprehensive view into your apps and infrastructure</li>
<li>CloudWatch collects via CloudWatch Agent:
<ul>
<li>API events from CloudTrail</li>
<li>networking events from VPC Flow Logs</li>
<li>app &#x26; system logs from EC2 instances</li>
</ul>
</li>
<li>Using a single CloudWatch log group ensures that system logs share the same retention, monitoring, and access control settings.</li>
<li>Lambda in each account can move log data from CW into Elasticsearch for indexing</li>
<li>Visualization with Kibana</li>
<li>CloudWatch log streams are a sequences of events from the same source, whereas a log group is a collection of log streams.</li>
<li>Logstash works well for collecting logs, but is usuallly paired with Elasticsearch for log monitoring and analysis.</li>
<li>AWS System Manager agents send status and execution info back to Systems Manager but NOT THE APPS LOGS, which the CW Logs agents are capable of sending.</li>
</ul>
<h4>want to monitor AWS resources, on premises resources, apps and services</h4>
<ul>
<li>CW console is a suitable service for selecting metrics and creating graphs</li>
<li>CW logs will allow you to log both AWS and on premises resources</li>
<li>CW Alarams will be suitable for alerts and notifications</li>
</ul>
<h3>CF</h3>
<h4>Best Practices</h4>
<ul>
<li>seperate stacks into individual, seperate logical components with dependencies on eachother</li>
<li>best way to link is with Exports and Imports</li>
<li>each CF template to be seperate file</li>
</ul>
<h4>upgrade database</h4>
<ul>
<li>major update - advances both minor and major - can take longer - compatability issue during &#x26; after possible</li>
<li>minor update - keeps the major version the same while advanic the minor</li>
<li>SourceDBInstanceIdentifier in CF template for a RDS Read Replica</li>
<li>rolling upgrade with read replicas</li>
</ul>
<h4>template creating VPC and a subnet forEach AZ w/in Region the Stack is created. Requires Region to be passed in stack as a parameter. Automate with intrinsic</h4>
<ul>
<li>Mappings Section - list each AZ in each Region</li>
<li>Resources use !Ref "AWS::Region" to return the Region</li>
<li>Fn::GetAZs to return a list of ALL AZs</li>
<li>FN:Select to choose each AZ from the list</li>
</ul>
<h3>CP</h3>
<ul>
<li>two versions of code require 2 pipelines</li>
<li>master to one production to another</li>
</ul>
<h4>GitHub Errors?</h4>
<ul>
<li>CP uses OAuth tokens to integrate with GitHub you might have revoked permissions on the Oath token for CP</li>
<li>The number of OAuth tokens is limited and if CP reaches that limit older tokens will stop working and actions in pipeline that rely upon that token will fail.</li>
<li>Try to manually configure one OAuth token as a personal access token and then configure all pipeline in your AWS account to use that token</li>
</ul>
<h3>CodeCommit</h3>
<ul>
<li>create IAM policy with required permissions and attach it to a  <code>developers</code> group</li>
</ul>
<h4>auto building PR's with a testing status badge</h4>
<ul>
<li>CW Events rule on POST/PUT to PR's with a target set to CB</li>
<li>2nd rule for success or fail of CB build it's target would be a lambda to update the PR</li>
</ul>
<h4>CICD pipeline for DR - Docker</h4>
<ul>
<li>use CB to acquire ECR credentials using the CLI helpers</li>
<li>build the image and push it into ECR</li>
<li>start another CD stage -> into ECS</li>
<li>CW Events does NOT support CodeDeploy as a target there CodeDeploy must be invoked by CP</li>
</ul>
<h4>compile release notes from CC</h4>
<ul>
<li>Setup CW Event rule to match CodeCommit repository events 'CodeCommit Repository State Change'</li>
<li>commit must be tagged for easy future reference</li>
<li>look for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge to master</li>
<li>Lmabda func use the CC API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket</li>
</ul>
<h3>AWS Config</h3>
<ul>
<li><em>AWS Config auditing capabilitiy provided with a timeline dashboard with compliance over time.</em></li>
</ul>
<h4>alerts of EBS unencryption</h4>
<ul>
<li>Config custom managed rule checking for EBS volume encryption.</li>
<li>CW Event rule alerts</li>
<li><em>SNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and <strong>NOT</strong> selectively for a given rule</em></li>
</ul>
<h3>StepFunctions</h3>
<h4>daily EBS backup workflow</h4>
<ul>
<li>StepFunction state machine with Lambda and CW Events is a solution</li>
<li>sf can coordinate the logic to automate the a snapshot flow</li>
</ul>
<h4>golden ami retrieve and inspect with AWS Inspector</h4>
<ul>
<li>using CW events schedule a SF which will launch a single EC2 instance from the AMI &#x26; tag it</li>
<li>SF kicks off AMI assessment template using AWS Inspector and the created tag.</li>
<li>cost effecitive to test on SINGLE EC2 instance - run assesment then terminate</li>
</ul>
<h3>Dynamo &#x26; Lambda &#x26; EC2</h3>
<h4>send logs before termination</h4>
<ul>
<li>Lifecycle hooks = custom actions by pausing instances as an ASG launches or terminates them.</li>
<li>while is <strong>WAIT</strong> state you can connect and download logs or other data</li>
<li>termination hook for ASG &#x26; create CW Event rule target -> lambda which invokes SSM Run Cmd to send log files from EC2 to S3</li>
<li>DynamoDB table to compile metadata index -> primary key = instance_id &#x26; sort key = datetime</li>
<li>S3 event integrate w/ lambda <code>PUT</code> -> triggers write to DynamoDB table</li>
</ul>
<h4>streams lambda throttle</h4>
<ul>
<li>fan-out pattern with lambda and SQS/SNS</li>
</ul>
<h3>ECS</h3>
<h4>send logs om a container on ECS</h4>
<ul>
<li>awslogs driver writes to CW logs natively</li>
<li>the EC2 should have proper IAM role to write to CWL</li>
<li>require <code>logs:CreateLogStream</code> and <code>logs:PutLogEvents</code> permission on the IAM role</li>
</ul>
<h3>AMIs ?</h3>
<h4>security hardened daily check for vulnerabilities</h4>
<ul>
<li>CWE on daily sched target = SF</li>
<li>SF -> EC2 instanve from AMI</li>
</ul>
<h3>Health Service</h3>
<h4>AWS_RISK_CREDENTIALS_EXPOSED</h4>
<ul>
<li>CW Event checking -> SF workflow to issue API calls to IAM, CloudTrail, and SNS (for alert)</li>
<li>Personal Health Dashboard service</li>
<li>if way to react to event will have retries and you want full audit trail of each workflow SF > Lambda</li>
</ul>
<h3>SSM</h3>
<h4>list of install software packages</h4>
<ul>
<li>
<p><em>SSM Agent can be installed and configured on EC2 instance, on prem, or a VM.</em></p>
</li>
<li>
<p>makes it possible for System Manager to update, manage, and configure these resources</p>
</li>
<li>
<p>SSM Inventory collects metadata from managed instances - data can be stored</p>
</li>
<li>
<p>Create CW Event rule yo trigger a lambda func on hourly basis. This can be set to do a comparison of the instances that sare running in EC2 and those tracked by SSM</p>
</li>
<li>
<p>An SSM automation cannont contain complex logic to hjandle failures, although it would provide an execution history.</p>
</li>
<li>
<p>An SSM automation documents defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs.</p>
</li>
<li>
<p>A document contains one or more steps that run in sequential order.</p>
</li>
<li>
<p>Each ste[ is built around a single action. The output of one step can be used as input in a later step</p>
</li>
<li>
<p>The process of running these actions and their steps is called the automation workflow.</p>
</li>
</ul>
<h3>EB</h3>
<h4>container_commands</h4>
<ul>
<li>
<p><code>container_commands</code> key to execute commands that affect your applciation source code</p>
</li>
<li>
<p>these run after the app and web server have been set up and the app version archive has been extracted</p>
</li>
<li>
<p>this is BEFORE the app is deployed</p>
</li>
<li>
<p><code>leader_only</code> to only run cmd when a test evaluates to true</p>
</li>
<li>
<p><code>commands</code> do not support leader_only attribute there use <code>container_commands</code></p>
</li>
<li>
<p>nginx comes default with Python in EB</p>
</li>
<li>
<p>Java can be deployed with Java Option, and the Java parameters can be specified in the 'Environment properties' section</p>
</li>
</ul>
<h4>blue/green with .NET/SQLServer/Github</h4>
<ul>
<li>Create an MSBuild container image with required tools for compliling .NET apps and push it to ECR</li>
<li>Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script</li>
<li>Have CP trigger AWS CodeBuild to use the MSBuild container image from ECR to compile the source code</li>
<li>Configure CB to then construct the .NET app</li>
<li>Also have CB create an executable to run the schuma update script</li>
<li>CP to trigger ACD to deploy .NET app to EB</li>
<li>CB invoke a PowerShell script to run the schema update executable</li>
</ul>
<h3>Trusted Advisor</h3>
<ul>
<li>checks infrastructure accorss all regions and provides summary of the results</li>
</ul>
<h4>check for low cpu ultization</h4>
<ul>
<li>check the low-utilized EC2 instances are on</li>
<li>create CW event that tracks the events created by TA use lambda as a target</li>
<li>the lambda should trigger SSM Automation document with manual approaval step</li>
<li>Upon approval, the SSM document proceeds with instance termination</li>
</ul>
<h3>Jenkins</h3>
<h4>Jenkins as build provider in CI/CD</h4>
<ul>
<li>CodePipeline plugin for Jenkins</li>
</ul>
<h3>AWS SAM</h3>
<h4>gradually shift customer traffic to the updated lambda in increments of 10% with 10 minutes between each increment until you are satisfied that it's working as expected</h4>
<ul>
<li>Define a DeploymentPreference of type 'Linear10PercentEvery10Minutes' in your AWS SAM template</li>
<li>Configure a post traffic hook Lambda function to run a sanity test that is invoked by CodeDeploy after traffic shifting completes</li>
</ul>
<h3>EC2</h3>
<h4>High Performance Computing (HPC) app on small number of EC2 placed in single AZ</h4>
<ul>
<li>Deploy the EC2 servers in a Cluster Placement Group</li>
<li>Cluster Placement Groups are recommended for application that benefit from low network latency, high network throughput, or both</li>
<li>also when the majority of the network traffic is between the instances in the group</li>
</ul>
<h3>Server Migration Service</h3>
<ul>
<li>migrate your servers to cloud?</li>
<li>current servers are built and configured automatically using Chef</li>
<li>Server Migration Service will bring your existing Chef managed machines into EC2 and to manage them with Chef the same way you have been on your in-house system</li>
</ul>
<h3>Kinesis Data Streams</h3>
<h4>Kinesis Data Streams - throughput lower then expected</h4>
<ul>
<li>Check the GetShartIterator, CreateStream, and DescribeStream <strong>Service Limits</strong></li>
<li>Use a SMall Producer with the Kinesis Producer Library, but using the PutRecords operation</li>
<li>Develop code using the ZKinesis Producer Library to put data onto the streams</li>
</ul>
<h4>slower than expected reading from the shards, but you are also seeing records being skipped - WHAT COULD CAUSE A SLOW DOWN IN READDING FROM A SHARD</h4>
<ul>
<li>for slow reading - it could be due to the maxRecords value being set too low</li>
<li>code logic which is calling processRecords being inefficient and causing high CPU usage or blocking</li>
<li>USUALLY PROCESSRECORDS CALLS HAVING BE UN HANDLED EXCEPTIONS</li>
</ul>
<h4>prcession incoming streaming data. GetRecords.IteratorAgeMilliseconds metric increases. How TO REDUCE THE TIME LAG IN THE STREAM</h4>
<ul>
<li>Increase parallelism by adding more shards per stream</li>
<li>Add more EC2 KCL consumers to allow each to process less shards per instance</li>
<li>An increased GetRecords.IteratorAgeMilliseconds metric means that either the KCL consumer cannot keep up processing the data from the Kinesis stream or there aren't enough shards in the stream</li>
</ul>
<h3>AWS CloudSearch</h3>
<h4>search contents of documents in S3</h4>
<ul>
<li>Implement Amazon CloudSearch</li>
<li>Setup Access policies</li>
<li>Configure your index</li>
<li>Creaete a search domain</li>
</ul>
<h3>Amazon Inspector</h3>
<h4>weekly scan of ports reachable from outside the VPC</h4>
<ul>
<li>Configure Netowork Assessments</li>
<li>Amazon Inspector is an automated security assessment service which will allow you to improve the security and compliance of your applcations.</li>
<li><strong>A network configuration analysis checks for any ports reachable from outside the VPC.</strong></li>
<li>THE AGENT IS REQUIRED</li>
</ul>
<h3>latency of static files currently shared EFS volume across all your app servers its to slow</h3>
<ul>
<li>Utilise Amazone CloudFront to optimise your static content</li>
<li>Use DynamoDB Global Tables to create a multi-master, multi-region data store for your application's back-end.</li>
<li>DynamoDB streams will propagate changes between the replicas so that users will have high performant and consistent app experience regardless of from where they access the site</li>
<li>MULTI REGUION REPLICATION AND MULTI-MASTER WRITES = GLOBAL TABLE in DYNAMODB</li>
</ul>
<h2>Importantly</h2>
<p>Jake Armijo <strong>|</strong> Full Stack Software Engineer
</p>
<p>Connect with me on <a href="https://www.linkedin.com/in/jake-armijo/">LinkedIn</a></p>
<p>Schedule a meeting with me on <a href="https://calendly.com/armijojake/meeting">Calendly</a></p>
</div></main><div class="layout_backToHome__1vZsp"><a href="/blog">‚Üê Back to Blog</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"aws-dev-ops-cert","contentHtml":"\u003ch2\u003eThe Domains of Exam\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDomain 1: SDLC Automation (16 questions)\u003c/strong\u003e -\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain 2: Configuration Management \u0026#x26; Infrastructure as Code (16 questions)\u003c/strong\u003e -\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain 3: Monitoring and Logging (11 questions)\u003c/strong\u003e -\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain 4: Policies and Standards Automation (9 questions)\u003c/strong\u003e -\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain 5: Incident and Event Response (13 questions)\u003c/strong\u003e -\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain 6: High Availability, Fault Tolerance, \u0026#x26; DR (10 questions)\u003c/strong\u003e -\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCD\u003c/h3\u003e\n\u003ch4\u003eblue/green is past but want to change to run one set of servers and to update everything at same time\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eEdit CD application deployment group and change the deployment type from blue/green to in-place\u003c/li\u003e\n\u003cli\u003eyou are able to edit the application deployment group in CodeDeploy, you do not have to create a new one.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003enew CC repo all but 1 can connect via SSh\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCC requires KMS. a policy might be attached to the already existing IAM user that DENIES KMS actions required\u003c/li\u003e\n\u003cli\u003eFor a new IAM user attach AWSCodeCommitFullAccess or another managed policy for CC access\u003c/li\u003e\n\u003cli\u003euser might have prev configured his local computer to use credential helper for CC. if so EDIT the .GITCONFIG file to remove the credential helper info from the file before using git creds\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eRDS test DB is isolated on private subnet. Want to run at night\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eAmazon VPC access in your CB project needs to be enabled\u003c/li\u003e\n\u003cli\u003eSpecify your VPC ID, subnets, and securty groups in your build project\u003c/li\u003e\n\u003cli\u003eset up CW Events to start pipeline on schedule\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eaws events put-rule --schedule-expression 'cron(10 3 ? * MON-FRI *)' --name IntegrationTests\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eCB DOES NOT SUPPORT assigning elastic IP addresses to the network interfaces that it creates, and auto-assigning a public IP address is not supported by EC2 for any network interfaces created outside EC2 instance launches\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eAWS_CODEBUILD_MAX_MEM_ALLOC defined in multiple parts\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003evalue in \u003cstrong\u003estart build operation\u003c/strong\u003e call takes HIGHEST precedence\u003c/li\u003e\n\u003cli\u003evalue in the \u003cstrong\u003ebuild project definition\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003evalue in the \u003cstrong\u003ebuild spec declaration\u003c/strong\u003e takes lowest precedence\u003c/li\u003e\n\u003cli\u003eDO NOT SET ENV VAR with 'CODEBUILD_'\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCW\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWith CloudTrail enabled create CW Event rule to track AWS API call via CloudTrail\u003c/li\u003e\n\u003cli\u003euse SNS as a target for notification\u003c/li\u003e\n\u003cli\u003ecreate a rule that triggers on an action by an AWS service that doesn not emit events, you can base the rule on API calls\u003c/li\u003e\n\u003cli\u003eDynamo Streams captures information but do not capture DeleteTable API calls, they only capture item level events\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eResolving the audit deficiency requires the creation of a centralized log monitoring capability for the AWS-based apps and infrastructure, and for certain on-premises systems that they interface with\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eElasticsearch is a fully managed service that lets you collect and analyze logs metrics, giving you a comprehensive view into your apps and infrastructure\u003c/li\u003e\n\u003cli\u003eCloudWatch collects via CloudWatch Agent:\n\u003cul\u003e\n\u003cli\u003eAPI events from CloudTrail\u003c/li\u003e\n\u003cli\u003enetworking events from VPC Flow Logs\u003c/li\u003e\n\u003cli\u003eapp \u0026#x26; system logs from EC2 instances\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUsing a single CloudWatch log group ensures that system logs share the same retention, monitoring, and access control settings.\u003c/li\u003e\n\u003cli\u003eLambda in each account can move log data from CW into Elasticsearch for indexing\u003c/li\u003e\n\u003cli\u003eVisualization with Kibana\u003c/li\u003e\n\u003cli\u003eCloudWatch log streams are a sequences of events from the same source, whereas a log group is a collection of log streams.\u003c/li\u003e\n\u003cli\u003eLogstash works well for collecting logs, but is usuallly paired with Elasticsearch for log monitoring and analysis.\u003c/li\u003e\n\u003cli\u003eAWS System Manager agents send status and execution info back to Systems Manager but NOT THE APPS LOGS, which the CW Logs agents are capable of sending.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003ewant to monitor AWS resources, on premises resources, apps and services\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCW console is a suitable service for selecting metrics and creating graphs\u003c/li\u003e\n\u003cli\u003eCW logs will allow you to log both AWS and on premises resources\u003c/li\u003e\n\u003cli\u003eCW Alarams will be suitable for alerts and notifications\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCF\u003c/h3\u003e\n\u003ch4\u003eBest Practices\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eseperate stacks into individual, seperate logical components with dependencies on eachother\u003c/li\u003e\n\u003cli\u003ebest way to link is with Exports and Imports\u003c/li\u003e\n\u003cli\u003eeach CF template to be seperate file\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eupgrade database\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003emajor update - advances both minor and major - can take longer - compatability issue during \u0026#x26; after possible\u003c/li\u003e\n\u003cli\u003eminor update - keeps the major version the same while advanic the minor\u003c/li\u003e\n\u003cli\u003eSourceDBInstanceIdentifier in CF template for a RDS Read Replica\u003c/li\u003e\n\u003cli\u003erolling upgrade with read replicas\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003etemplate creating VPC and a subnet forEach AZ w/in Region the Stack is created. Requires Region to be passed in stack as a parameter. Automate with intrinsic\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eMappings Section - list each AZ in each Region\u003c/li\u003e\n\u003cli\u003eResources use !Ref \"AWS::Region\" to return the Region\u003c/li\u003e\n\u003cli\u003eFn::GetAZs to return a list of ALL AZs\u003c/li\u003e\n\u003cli\u003eFN:Select to choose each AZ from the list\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCP\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003etwo versions of code require 2 pipelines\u003c/li\u003e\n\u003cli\u003emaster to one production to another\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eGitHub Errors?\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCP uses OAuth tokens to integrate with GitHub you might have revoked permissions on the Oath token for CP\u003c/li\u003e\n\u003cli\u003eThe number of OAuth tokens is limited and if CP reaches that limit older tokens will stop working and actions in pipeline that rely upon that token will fail.\u003c/li\u003e\n\u003cli\u003eTry to manually configure one OAuth token as a personal access token and then configure all pipeline in your AWS account to use that token\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCodeCommit\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecreate IAM policy with required permissions and attach it to a  \u003ccode\u003edevelopers\u003c/code\u003e group\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eauto building PR's with a testing status badge\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCW Events rule on POST/PUT to PR's with a target set to CB\u003c/li\u003e\n\u003cli\u003e2nd rule for success or fail of CB build it's target would be a lambda to update the PR\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eCICD pipeline for DR - Docker\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003euse CB to acquire ECR credentials using the CLI helpers\u003c/li\u003e\n\u003cli\u003ebuild the image and push it into ECR\u003c/li\u003e\n\u003cli\u003estart another CD stage -\u003e into ECS\u003c/li\u003e\n\u003cli\u003eCW Events does NOT support CodeDeploy as a target there CodeDeploy must be invoked by CP\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003ecompile release notes from CC\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSetup CW Event rule to match CodeCommit repository events 'CodeCommit Repository State Change'\u003c/li\u003e\n\u003cli\u003ecommit must be tagged for easy future reference\u003c/li\u003e\n\u003cli\u003elook for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge to master\u003c/li\u003e\n\u003cli\u003eLmabda func use the CC API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAWS Config\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eAWS Config auditing capabilitiy provided with a timeline dashboard with compliance over time.\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003ealerts of EBS unencryption\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eConfig custom managed rule checking for EBS volume encryption.\u003c/li\u003e\n\u003cli\u003eCW Event rule alerts\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eSNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and \u003cstrong\u003eNOT\u003c/strong\u003e selectively for a given rule\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eStepFunctions\u003c/h3\u003e\n\u003ch4\u003edaily EBS backup workflow\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eStepFunction state machine with Lambda and CW Events is a solution\u003c/li\u003e\n\u003cli\u003esf can coordinate the logic to automate the a snapshot flow\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003egolden ami retrieve and inspect with AWS Inspector\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eusing CW events schedule a SF which will launch a single EC2 instance from the AMI \u0026#x26; tag it\u003c/li\u003e\n\u003cli\u003eSF kicks off AMI assessment template using AWS Inspector and the created tag.\u003c/li\u003e\n\u003cli\u003ecost effecitive to test on SINGLE EC2 instance - run assesment then terminate\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eDynamo \u0026#x26; Lambda \u0026#x26; EC2\u003c/h3\u003e\n\u003ch4\u003esend logs before termination\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eLifecycle hooks = custom actions by pausing instances as an ASG launches or terminates them.\u003c/li\u003e\n\u003cli\u003ewhile is \u003cstrong\u003eWAIT\u003c/strong\u003e state you can connect and download logs or other data\u003c/li\u003e\n\u003cli\u003etermination hook for ASG \u0026#x26; create CW Event rule target -\u003e lambda which invokes SSM Run Cmd to send log files from EC2 to S3\u003c/li\u003e\n\u003cli\u003eDynamoDB table to compile metadata index -\u003e primary key = instance_id \u0026#x26; sort key = datetime\u003c/li\u003e\n\u003cli\u003eS3 event integrate w/ lambda \u003ccode\u003ePUT\u003c/code\u003e -\u003e triggers write to DynamoDB table\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003estreams lambda throttle\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003efan-out pattern with lambda and SQS/SNS\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eECS\u003c/h3\u003e\n\u003ch4\u003esend logs om a container on ECS\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eawslogs driver writes to CW logs natively\u003c/li\u003e\n\u003cli\u003ethe EC2 should have proper IAM role to write to CWL\u003c/li\u003e\n\u003cli\u003erequire \u003ccode\u003elogs:CreateLogStream\u003c/code\u003e and \u003ccode\u003elogs:PutLogEvents\u003c/code\u003e permission on the IAM role\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAMIs ?\u003c/h3\u003e\n\u003ch4\u003esecurity hardened daily check for vulnerabilities\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCWE on daily sched target = SF\u003c/li\u003e\n\u003cli\u003eSF -\u003e EC2 instanve from AMI\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eHealth Service\u003c/h3\u003e\n\u003ch4\u003eAWS_RISK_CREDENTIALS_EXPOSED\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCW Event checking -\u003e SF workflow to issue API calls to IAM, CloudTrail, and SNS (for alert)\u003c/li\u003e\n\u003cli\u003ePersonal Health Dashboard service\u003c/li\u003e\n\u003cli\u003eif way to react to event will have retries and you want full audit trail of each workflow SF \u003e Lambda\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eSSM\u003c/h3\u003e\n\u003ch4\u003elist of install software packages\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eSSM Agent can be installed and configured on EC2 instance, on prem, or a VM.\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003emakes it possible for System Manager to update, manage, and configure these resources\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSSM Inventory collects metadata from managed instances - data can be stored\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate CW Event rule yo trigger a lambda func on hourly basis. This can be set to do a comparison of the instances that sare running in EC2 and those tracked by SSM\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAn SSM automation cannont contain complex logic to hjandle failures, although it would provide an execution history.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAn SSM automation documents defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA document contains one or more steps that run in sequential order.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEach ste[ is built around a single action. The output of one step can be used as input in a later step\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe process of running these actions and their steps is called the automation workflow.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eEB\u003c/h3\u003e\n\u003ch4\u003econtainer_commands\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003econtainer_commands\u003c/code\u003e key to execute commands that affect your applciation source code\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ethese run after the app and web server have been set up and the app version archive has been extracted\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ethis is BEFORE the app is deployed\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eleader_only\u003c/code\u003e to only run cmd when a test evaluates to true\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ecommands\u003c/code\u003e do not support leader_only attribute there use \u003ccode\u003econtainer_commands\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003enginx comes default with Python in EB\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJava can be deployed with Java Option, and the Java parameters can be specified in the 'Environment properties' section\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eblue/green with .NET/SQLServer/Github\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCreate an MSBuild container image with required tools for compliling .NET apps and push it to ECR\u003c/li\u003e\n\u003cli\u003eConfigure AWS Code Pipeline to fetch the latest GitHub code and schema update script\u003c/li\u003e\n\u003cli\u003eHave CP trigger AWS CodeBuild to use the MSBuild container image from ECR to compile the source code\u003c/li\u003e\n\u003cli\u003eConfigure CB to then construct the .NET app\u003c/li\u003e\n\u003cli\u003eAlso have CB create an executable to run the schuma update script\u003c/li\u003e\n\u003cli\u003eCP to trigger ACD to deploy .NET app to EB\u003c/li\u003e\n\u003cli\u003eCB invoke a PowerShell script to run the schema update executable\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eTrusted Advisor\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003echecks infrastructure accorss all regions and provides summary of the results\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003echeck for low cpu ultization\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003echeck the low-utilized EC2 instances are on\u003c/li\u003e\n\u003cli\u003ecreate CW event that tracks the events created by TA use lambda as a target\u003c/li\u003e\n\u003cli\u003ethe lambda should trigger SSM Automation document with manual approaval step\u003c/li\u003e\n\u003cli\u003eUpon approval, the SSM document proceeds with instance termination\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eJenkins\u003c/h3\u003e\n\u003ch4\u003eJenkins as build provider in CI/CD\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCodePipeline plugin for Jenkins\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAWS SAM\u003c/h3\u003e\n\u003ch4\u003egradually shift customer traffic to the updated lambda in increments of 10% with 10 minutes between each increment until you are satisfied that it's working as expected\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eDefine a DeploymentPreference of type 'Linear10PercentEvery10Minutes' in your AWS SAM template\u003c/li\u003e\n\u003cli\u003eConfigure a post traffic hook Lambda function to run a sanity test that is invoked by CodeDeploy after traffic shifting completes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eEC2\u003c/h3\u003e\n\u003ch4\u003eHigh Performance Computing (HPC) app on small number of EC2 placed in single AZ\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eDeploy the EC2 servers in a Cluster Placement Group\u003c/li\u003e\n\u003cli\u003eCluster Placement Groups are recommended for application that benefit from low network latency, high network throughput, or both\u003c/li\u003e\n\u003cli\u003ealso when the majority of the network traffic is between the instances in the group\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eServer Migration Service\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003emigrate your servers to cloud?\u003c/li\u003e\n\u003cli\u003ecurrent servers are built and configured automatically using Chef\u003c/li\u003e\n\u003cli\u003eServer Migration Service will bring your existing Chef managed machines into EC2 and to manage them with Chef the same way you have been on your in-house system\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eKinesis Data Streams\u003c/h3\u003e\n\u003ch4\u003eKinesis Data Streams - throughput lower then expected\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCheck the GetShartIterator, CreateStream, and DescribeStream \u003cstrong\u003eService Limits\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eUse a SMall Producer with the Kinesis Producer Library, but using the PutRecords operation\u003c/li\u003e\n\u003cli\u003eDevelop code using the ZKinesis Producer Library to put data onto the streams\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eslower than expected reading from the shards, but you are also seeing records being skipped - WHAT COULD CAUSE A SLOW DOWN IN READDING FROM A SHARD\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003efor slow reading - it could be due to the maxRecords value being set too low\u003c/li\u003e\n\u003cli\u003ecode logic which is calling processRecords being inefficient and causing high CPU usage or blocking\u003c/li\u003e\n\u003cli\u003eUSUALLY PROCESSRECORDS CALLS HAVING BE UN HANDLED EXCEPTIONS\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eprcession incoming streaming data. GetRecords.IteratorAgeMilliseconds metric increases. How TO REDUCE THE TIME LAG IN THE STREAM\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eIncrease parallelism by adding more shards per stream\u003c/li\u003e\n\u003cli\u003eAdd more EC2 KCL consumers to allow each to process less shards per instance\u003c/li\u003e\n\u003cli\u003eAn increased GetRecords.IteratorAgeMilliseconds metric means that either the KCL consumer cannot keep up processing the data from the Kinesis stream or there aren't enough shards in the stream\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAWS CloudSearch\u003c/h3\u003e\n\u003ch4\u003esearch contents of documents in S3\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eImplement Amazon CloudSearch\u003c/li\u003e\n\u003cli\u003eSetup Access policies\u003c/li\u003e\n\u003cli\u003eConfigure your index\u003c/li\u003e\n\u003cli\u003eCreaete a search domain\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eAmazon Inspector\u003c/h3\u003e\n\u003ch4\u003eweekly scan of ports reachable from outside the VPC\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eConfigure Netowork Assessments\u003c/li\u003e\n\u003cli\u003eAmazon Inspector is an automated security assessment service which will allow you to improve the security and compliance of your applcations.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eA network configuration analysis checks for any ports reachable from outside the VPC.\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eTHE AGENT IS REQUIRED\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003elatency of static files currently shared EFS volume across all your app servers its to slow\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUtilise Amazone CloudFront to optimise your static content\u003c/li\u003e\n\u003cli\u003eUse DynamoDB Global Tables to create a multi-master, multi-region data store for your application's back-end.\u003c/li\u003e\n\u003cli\u003eDynamoDB streams will propagate changes between the replicas so that users will have high performant and consistent app experience regardless of from where they access the site\u003c/li\u003e\n\u003cli\u003eMULTI REGUION REPLICATION AND MULTI-MASTER WRITES = GLOBAL TABLE in DYNAMODB\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eImportantly\u003c/h2\u003e\n\u003cp\u003eJake Armijo \u003cstrong\u003e|\u003c/strong\u003e Full Stack Software Engineer\n\u003c/p\u003e\n\u003cp\u003eConnect with me on \u003ca href=\"https://www.linkedin.com/in/jake-armijo/\"\u003eLinkedIn\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSchedule a meeting with me on \u003ca href=\"https://calendly.com/armijojake/meeting\"\u003eCalendly\u003c/a\u003e\u003c/p\u003e\n","title":"AWS DOP-C01","date":"2022-08-23","category":"Guidance/Advice","template":"blog-post","tags":"AWS, Exam, DEV OPS"}},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"aws-dev-ops-prof"},"buildId":"FxYUuXgttL70YrK1Pz9qW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>