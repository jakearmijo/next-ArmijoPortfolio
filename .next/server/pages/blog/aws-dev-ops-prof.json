{"pageProps":{"postData":{"id":"aws-dev-ops-cert","contentHtml":"<h2>The Domains of Exam</h2>\n<ul>\n<li><strong>Domain 1: SDLC Automation (16 questions)</strong> -</li>\n<li><strong>Domain 2: Configuration Management &#x26; Infrastructure as Code (16 questions)</strong> -</li>\n<li><strong>Domain 3: Monitoring and Logging (11 questions)</strong> -</li>\n<li><strong>Domain 4: Policies and Standards Automation (9 questions)</strong> -</li>\n<li><strong>Domain 5: Incident and Event Response (13 questions)</strong> -</li>\n<li><strong>Domain 6: High Availability, Fault Tolerance, &#x26; DR (10 questions)</strong> -</li>\n</ul>\n<h3>CD</h3>\n<h4>blue/green is past but want to change to run one set of servers and to update everything at same time</h4>\n<ul>\n<li>Edit CD application deployment group and change the deployment type from blue/green to in-place</li>\n<li>you are able to edit the application deployment group in CodeDeploy, you do not have to create a new one.</li>\n</ul>\n<h4>new CC repo all but 1 can connect via SSh</h4>\n<ul>\n<li>CC requires KMS. a policy might be attached to the already existing IAM user that DENIES KMS actions required</li>\n<li>For a new IAM user attach AWSCodeCommitFullAccess or another managed policy for CC access</li>\n<li>user might have prev configured his local computer to use credential helper for CC. if so EDIT the .GITCONFIG file to remove the credential helper info from the file before using git creds</li>\n</ul>\n<h4>RDS test DB is isolated on private subnet. Want to run at night</h4>\n<ul>\n<li>Amazon VPC access in your CB project needs to be enabled</li>\n<li>Specify your VPC ID, subnets, and securty groups in your build project</li>\n<li>set up CW Events to start pipeline on schedule</li>\n<li><code>aws events put-rule --schedule-expression 'cron(10 3 ? * MON-FRI *)' --name IntegrationTests</code></li>\n<li>CB DOES NOT SUPPORT assigning elastic IP addresses to the network interfaces that it creates, and auto-assigning a public IP address is not supported by EC2 for any network interfaces created outside EC2 instance launches</li>\n</ul>\n<h4>AWS_CODEBUILD_MAX_MEM_ALLOC defined in multiple parts</h4>\n<ul>\n<li>value in <strong>start build operation</strong> call takes HIGHEST precedence</li>\n<li>value in the <strong>build project definition</strong></li>\n<li>value in the <strong>build spec declaration</strong> takes lowest precedence</li>\n<li>DO NOT SET ENV VAR with 'CODEBUILD_'</li>\n</ul>\n<h3>CW</h3>\n<ul>\n<li>With CloudTrail enabled create CW Event rule to track AWS API call via CloudTrail</li>\n<li>use SNS as a target for notification</li>\n<li>create a rule that triggers on an action by an AWS service that doesn not emit events, you can base the rule on API calls</li>\n<li>Dynamo Streams captures information but do not capture DeleteTable API calls, they only capture item level events</li>\n</ul>\n<h4>Resolving the audit deficiency requires the creation of a centralized log monitoring capability for the AWS-based apps and infrastructure, and for certain on-premises systems that they interface with</h4>\n<ul>\n<li>Elasticsearch is a fully managed service that lets you collect and analyze logs metrics, giving you a comprehensive view into your apps and infrastructure</li>\n<li>CloudWatch collects via CloudWatch Agent:\n<ul>\n<li>API events from CloudTrail</li>\n<li>networking events from VPC Flow Logs</li>\n<li>app &#x26; system logs from EC2 instances</li>\n</ul>\n</li>\n<li>Using a single CloudWatch log group ensures that system logs share the same retention, monitoring, and access control settings.</li>\n<li>Lambda in each account can move log data from CW into Elasticsearch for indexing</li>\n<li>Visualization with Kibana</li>\n<li>CloudWatch log streams are a sequences of events from the same source, whereas a log group is a collection of log streams.</li>\n<li>Logstash works well for collecting logs, but is usuallly paired with Elasticsearch for log monitoring and analysis.</li>\n<li>AWS System Manager agents send status and execution info back to Systems Manager but NOT THE APPS LOGS, which the CW Logs agents are capable of sending.</li>\n</ul>\n<h4>want to monitor AWS resources, on premises resources, apps and services</h4>\n<ul>\n<li>CW console is a suitable service for selecting metrics and creating graphs</li>\n<li>CW logs will allow you to log both AWS and on premises resources</li>\n<li>CW Alarams will be suitable for alerts and notifications</li>\n</ul>\n<h3>CF</h3>\n<h4>Best Practices</h4>\n<ul>\n<li>seperate stacks into individual, seperate logical components with dependencies on eachother</li>\n<li>best way to link is with Exports and Imports</li>\n<li>each CF template to be seperate file</li>\n</ul>\n<h4>upgrade database</h4>\n<ul>\n<li>major update - advances both minor and major - can take longer - compatability issue during &#x26; after possible</li>\n<li>minor update - keeps the major version the same while advanic the minor</li>\n<li>SourceDBInstanceIdentifier in CF template for a RDS Read Replica</li>\n<li>rolling upgrade with read replicas</li>\n</ul>\n<h4>template creating VPC and a subnet forEach AZ w/in Region the Stack is created. Requires Region to be passed in stack as a parameter. Automate with intrinsic</h4>\n<ul>\n<li>Mappings Section - list each AZ in each Region</li>\n<li>Resources use !Ref \"AWS::Region\" to return the Region</li>\n<li>Fn::GetAZs to return a list of ALL AZs</li>\n<li>FN:Select to choose each AZ from the list</li>\n</ul>\n<h3>CP</h3>\n<ul>\n<li>two versions of code require 2 pipelines</li>\n<li>master to one production to another</li>\n</ul>\n<h4>GitHub Errors?</h4>\n<ul>\n<li>CP uses OAuth tokens to integrate with GitHub you might have revoked permissions on the Oath token for CP</li>\n<li>The number of OAuth tokens is limited and if CP reaches that limit older tokens will stop working and actions in pipeline that rely upon that token will fail.</li>\n<li>Try to manually configure one OAuth token as a personal access token and then configure all pipeline in your AWS account to use that token</li>\n</ul>\n<h3>CodeCommit</h3>\n<ul>\n<li>create IAM policy with required permissions and attach it to a  <code>developers</code> group</li>\n</ul>\n<h4>auto building PR's with a testing status badge</h4>\n<ul>\n<li>CW Events rule on POST/PUT to PR's with a target set to CB</li>\n<li>2nd rule for success or fail of CB build it's target would be a lambda to update the PR</li>\n</ul>\n<h4>CICD pipeline for DR - Docker</h4>\n<ul>\n<li>use CB to acquire ECR credentials using the CLI helpers</li>\n<li>build the image and push it into ECR</li>\n<li>start another CD stage -> into ECS</li>\n<li>CW Events does NOT support CodeDeploy as a target there CodeDeploy must be invoked by CP</li>\n</ul>\n<h4>compile release notes from CC</h4>\n<ul>\n<li>Setup CW Event rule to match CodeCommit repository events 'CodeCommit Repository State Change'</li>\n<li>commit must be tagged for easy future reference</li>\n<li>look for 'referenceCreated' events with a 'tag' referenceType that are created when a production release is tagged after a merge to master</li>\n<li>Lmabda func use the CC API to retrieve that release commit message and store it in a static website hosting enabled S3 bucket</li>\n</ul>\n<h3>AWS Config</h3>\n<ul>\n<li><em>AWS Config auditing capabilitiy provided with a timeline dashboard with compliance over time.</em></li>\n</ul>\n<h4>alerts of EBS unencryption</h4>\n<ul>\n<li>Config custom managed rule checking for EBS volume encryption.</li>\n<li>CW Event rule alerts</li>\n<li><em>SNS topics when directly integrated with Config can only be used to stream all the notifications and configuration changes and <strong>NOT</strong> selectively for a given rule</em></li>\n</ul>\n<h3>StepFunctions</h3>\n<h4>daily EBS backup workflow</h4>\n<ul>\n<li>StepFunction state machine with Lambda and CW Events is a solution</li>\n<li>sf can coordinate the logic to automate the a snapshot flow</li>\n</ul>\n<h4>golden ami retrieve and inspect with AWS Inspector</h4>\n<ul>\n<li>using CW events schedule a SF which will launch a single EC2 instance from the AMI &#x26; tag it</li>\n<li>SF kicks off AMI assessment template using AWS Inspector and the created tag.</li>\n<li>cost effecitive to test on SINGLE EC2 instance - run assesment then terminate</li>\n</ul>\n<h3>Dynamo &#x26; Lambda &#x26; EC2</h3>\n<h4>send logs before termination</h4>\n<ul>\n<li>Lifecycle hooks = custom actions by pausing instances as an ASG launches or terminates them.</li>\n<li>while is <strong>WAIT</strong> state you can connect and download logs or other data</li>\n<li>termination hook for ASG &#x26; create CW Event rule target -> lambda which invokes SSM Run Cmd to send log files from EC2 to S3</li>\n<li>DynamoDB table to compile metadata index -> primary key = instance_id &#x26; sort key = datetime</li>\n<li>S3 event integrate w/ lambda <code>PUT</code> -> triggers write to DynamoDB table</li>\n</ul>\n<h4>streams lambda throttle</h4>\n<ul>\n<li>fan-out pattern with lambda and SQS/SNS</li>\n</ul>\n<h3>ECS</h3>\n<h4>send logs om a container on ECS</h4>\n<ul>\n<li>awslogs driver writes to CW logs natively</li>\n<li>the EC2 should have proper IAM role to write to CWL</li>\n<li>require <code>logs:CreateLogStream</code> and <code>logs:PutLogEvents</code> permission on the IAM role</li>\n</ul>\n<h3>AMIs ?</h3>\n<h4>security hardened daily check for vulnerabilities</h4>\n<ul>\n<li>CWE on daily sched target = SF</li>\n<li>SF -> EC2 instanve from AMI</li>\n</ul>\n<h3>Health Service</h3>\n<h4>AWS_RISK_CREDENTIALS_EXPOSED</h4>\n<ul>\n<li>CW Event checking -> SF workflow to issue API calls to IAM, CloudTrail, and SNS (for alert)</li>\n<li>Personal Health Dashboard service</li>\n<li>if way to react to event will have retries and you want full audit trail of each workflow SF > Lambda</li>\n</ul>\n<h3>SSM</h3>\n<h4>list of install software packages</h4>\n<ul>\n<li>\n<p><em>SSM Agent can be installed and configured on EC2 instance, on prem, or a VM.</em></p>\n</li>\n<li>\n<p>makes it possible for System Manager to update, manage, and configure these resources</p>\n</li>\n<li>\n<p>SSM Inventory collects metadata from managed instances - data can be stored</p>\n</li>\n<li>\n<p>Create CW Event rule yo trigger a lambda func on hourly basis. This can be set to do a comparison of the instances that sare running in EC2 and those tracked by SSM</p>\n</li>\n<li>\n<p>An SSM automation cannont contain complex logic to hjandle failures, although it would provide an execution history.</p>\n</li>\n<li>\n<p>An SSM automation documents defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation execution runs.</p>\n</li>\n<li>\n<p>A document contains one or more steps that run in sequential order.</p>\n</li>\n<li>\n<p>Each ste[ is built around a single action. The output of one step can be used as input in a later step</p>\n</li>\n<li>\n<p>The process of running these actions and their steps is called the automation workflow.</p>\n</li>\n</ul>\n<h3>EB</h3>\n<h4>container_commands</h4>\n<ul>\n<li>\n<p><code>container_commands</code> key to execute commands that affect your applciation source code</p>\n</li>\n<li>\n<p>these run after the app and web server have been set up and the app version archive has been extracted</p>\n</li>\n<li>\n<p>this is BEFORE the app is deployed</p>\n</li>\n<li>\n<p><code>leader_only</code> to only run cmd when a test evaluates to true</p>\n</li>\n<li>\n<p><code>commands</code> do not support leader_only attribute there use <code>container_commands</code></p>\n</li>\n<li>\n<p>nginx comes default with Python in EB</p>\n</li>\n<li>\n<p>Java can be deployed with Java Option, and the Java parameters can be specified in the 'Environment properties' section</p>\n</li>\n</ul>\n<h4>blue/green with .NET/SQLServer/Github</h4>\n<ul>\n<li>Create an MSBuild container image with required tools for compliling .NET apps and push it to ECR</li>\n<li>Configure AWS Code Pipeline to fetch the latest GitHub code and schema update script</li>\n<li>Have CP trigger AWS CodeBuild to use the MSBuild container image from ECR to compile the source code</li>\n<li>Configure CB to then construct the .NET app</li>\n<li>Also have CB create an executable to run the schuma update script</li>\n<li>CP to trigger ACD to deploy .NET app to EB</li>\n<li>CB invoke a PowerShell script to run the schema update executable</li>\n</ul>\n<h3>Trusted Advisor</h3>\n<ul>\n<li>checks infrastructure accorss all regions and provides summary of the results</li>\n</ul>\n<h4>check for low cpu ultization</h4>\n<ul>\n<li>check the low-utilized EC2 instances are on</li>\n<li>create CW event that tracks the events created by TA use lambda as a target</li>\n<li>the lambda should trigger SSM Automation document with manual approaval step</li>\n<li>Upon approval, the SSM document proceeds with instance termination</li>\n</ul>\n<h3>Jenkins</h3>\n<h4>Jenkins as build provider in CI/CD</h4>\n<ul>\n<li>CodePipeline plugin for Jenkins</li>\n</ul>\n<h3>AWS SAM</h3>\n<h4>gradually shift customer traffic to the updated lambda in increments of 10% with 10 minutes between each increment until you are satisfied that it's working as expected</h4>\n<ul>\n<li>Define a DeploymentPreference of type 'Linear10PercentEvery10Minutes' in your AWS SAM template</li>\n<li>Configure a post traffic hook Lambda function to run a sanity test that is invoked by CodeDeploy after traffic shifting completes</li>\n</ul>\n<h3>EC2</h3>\n<h4>High Performance Computing (HPC) app on small number of EC2 placed in single AZ</h4>\n<ul>\n<li>Deploy the EC2 servers in a Cluster Placement Group</li>\n<li>Cluster Placement Groups are recommended for application that benefit from low network latency, high network throughput, or both</li>\n<li>also when the majority of the network traffic is between the instances in the group</li>\n</ul>\n<h3>Server Migration Service</h3>\n<ul>\n<li>migrate your servers to cloud?</li>\n<li>current servers are built and configured automatically using Chef</li>\n<li>Server Migration Service will bring your existing Chef managed machines into EC2 and to manage them with Chef the same way you have been on your in-house system</li>\n</ul>\n<h3>Kinesis Data Streams</h3>\n<h4>Kinesis Data Streams - throughput lower then expected</h4>\n<ul>\n<li>Check the GetShartIterator, CreateStream, and DescribeStream <strong>Service Limits</strong></li>\n<li>Use a SMall Producer with the Kinesis Producer Library, but using the PutRecords operation</li>\n<li>Develop code using the ZKinesis Producer Library to put data onto the streams</li>\n</ul>\n<h4>slower than expected reading from the shards, but you are also seeing records being skipped - WHAT COULD CAUSE A SLOW DOWN IN READDING FROM A SHARD</h4>\n<ul>\n<li>for slow reading - it could be due to the maxRecords value being set too low</li>\n<li>code logic which is calling processRecords being inefficient and causing high CPU usage or blocking</li>\n<li>USUALLY PROCESSRECORDS CALLS HAVING BE UN HANDLED EXCEPTIONS</li>\n</ul>\n<h4>prcession incoming streaming data. GetRecords.IteratorAgeMilliseconds metric increases. How TO REDUCE THE TIME LAG IN THE STREAM</h4>\n<ul>\n<li>Increase parallelism by adding more shards per stream</li>\n<li>Add more EC2 KCL consumers to allow each to process less shards per instance</li>\n<li>An increased GetRecords.IteratorAgeMilliseconds metric means that either the KCL consumer cannot keep up processing the data from the Kinesis stream or there aren't enough shards in the stream</li>\n</ul>\n<h3>AWS CloudSearch</h3>\n<h4>search contents of documents in S3</h4>\n<ul>\n<li>Implement Amazon CloudSearch</li>\n<li>Setup Access policies</li>\n<li>Configure your index</li>\n<li>Creaete a search domain</li>\n</ul>\n<h3>Amazon Inspector</h3>\n<h4>weekly scan of ports reachable from outside the VPC</h4>\n<ul>\n<li>Configure Netowork Assessments</li>\n<li>Amazon Inspector is an automated security assessment service which will allow you to improve the security and compliance of your applcations.</li>\n<li><strong>A network configuration analysis checks for any ports reachable from outside the VPC.</strong></li>\n<li>THE AGENT IS REQUIRED</li>\n</ul>\n<h3>latency of static files currently shared EFS volume across all your app servers its to slow</h3>\n<ul>\n<li>Utilise Amazone CloudFront to optimise your static content</li>\n<li>Use DynamoDB Global Tables to create a multi-master, multi-region data store for your application's back-end.</li>\n<li>DynamoDB streams will propagate changes between the replicas so that users will have high performant and consistent app experience regardless of from where they access the site</li>\n<li>MULTI REGUION REPLICATION AND MULTI-MASTER WRITES = GLOBAL TABLE in DYNAMODB</li>\n</ul>\n<h2>Importantly</h2>\n<p>Jake Armijo <strong>|</strong> Full Stack Software Engineer\n</p>\n<p>Connect with me on <a href=\"https://www.linkedin.com/in/jake-armijo/\">LinkedIn</a></p>\n<p>Schedule a meeting with me on <a href=\"https://calendly.com/armijojake/meeting\">Calendly</a></p>\n","title":"AWS DOP-C01","date":"2022-08-23","category":"Guidance/Advice","template":"blog-post","tags":"AWS, Exam, DEV OPS"}},"__N_SSG":true}